---
title: "Morphological profiling workflows"
author: "Cytomining Hackathon attendees"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Morphological profiling workflows}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(magrittr)
library(dplyr)
futile.logger::flog.threshold(futile.logger::WARN)
```


## Load data
First, load the data, which is stored in a database backend 

```{r}
fixture <- '~/tmp/BBBC021.sqlite'

if (!file.exists(fixture)) {
  download.file("http://www.broadinstitute.org/~shsingh/BBBC021.sqlite", 
              destfile = fixture, 
              method = "curl", 
              quiet = FALSE, mode = "w",
              cacheOK = TRUE,
              extra = getOption("download.file.extra"))
}

db <- src_sqlite(path = fixture)
```


```{r}
images <-
  tbl(src = db, "supplement_Image") 

objects <- 
  tbl(src = db, "supplement_Object") 

metadata <- 
  tbl(src = db, "supplement_GroundTruth") %>%
  rename(Image_Metadata_Compound = compound,
         Image_Metadata_Concentration = concentration,
         Image_Metadata_MOA = moa) 

images %<>%
  inner_join(
    metadata,
    by = c("Image_Metadata_Compound", "Image_Metadata_Concentration")
  )

data <-
  inner_join(images,
             objects,
             by = c("TableNumber", "ImageNumber")
  ) %>%
  compute()

```


How many rows does this table have?

```{r}
data %>%
  dplyr::tally() %>%
  knitr::kable()
```

All the code after this belongs to a different dataset but can provide some 
clues on how to do the analysis

```{r}
qc_cols <- c("q_debris")

group_cols <-
  c("g_plate",
    "g_well",
    "g_image",
    "g_pattern",
    "g_channel")

feature_cols <-
  colnames(measurements) %>%
  stringr::str_subset("^m_")

measurements %<>%
  dplyr::select(one_of(c(group_cols, qc_cols, feature_cols)))

```

## Clean

Let's remove cells that come from images that were marked as having debris

```{r}
debris_removed <-
  measurements %>% dplyr::filter(q_debris == 0)
```

Then, remove cells where all the measurements are NA's (TODO: explain why this
may be needed)

```{r}
na_rows_removed <-
  cytominr::drop_na_rows(
    population = debris_removed,
    variables = feature_cols
  ) %>%
  dplyr::compute()
```

### Normalize 

We need to normalize the data so that

- features are on the same scale

- plate-to-plate variation is reduced

The default for doing this is `standardization`. Here, we take all the cells
from control wells in the experiment (this is where the external metadata gets
used) and compute normalizations parameters from that (in this case, just the
mean and s.d.) and then apply it to the whole dataset (i.e. the population)

```{r}
normalized <-
  cytominr::normalize(
    population = na_rows_removed,
    variables = feature_cols,
    strata =  c("g_plate", "g_pattern", "g_channel"),
    sample =
      na_rows_removed %>%
      dplyr::inner_join(
        ext_metadata %>% dplyr::filter(Type == "ctrl") %>%
          dplyr::select(g_well)
      )
  )
```

In some cases, we may have features that have no variance at all (e.g. Euler 
number). If these features have not already been removed by this stage, the 
standardization step will results in all values for that feature being NA (
because s.d. = 0). Lets remove them:

First, count how many cells have NA values per feature:

```{r}
na_frequency <-
  cytominr::count_na_rows(
    population = normalized,
    variables = feature_cols)

na_frequency %>%
  tidyr::gather(feature, na_count) %>%
  knitr::kable()

```

As it turns out, no feature has NA in this example. 
But lets run this cleaning operation  anyway (no features will be dropped)

```{r}

cleaned <-
  cytominr::select(
    population = normalized,
    variables = feature_cols,
    operation = "drop_na_columns"
)
```

## Transform

Tranform the data so that assumptions we may later make about the data
distribution are satisfied (e.g. Gaussianity). The default here is 
`generalized_log`. (TODO: explain this further) 

```{r}
transformed <-
  cytominr::transform(
    population = cleaned,
    variables = feature_cols
  )
```

## Select features

Finally, we typically perform feature selection on the data (TODO: explain 
further). Feature selection is an expensive operation, so we usually want to 
train the feature selection model on a sample of the dataset. Here, we choose
to aggregate the data instead of sampling it (i.e. collapse it to per-well
aggregates)

```{r}
aggregated <-
  cytominr::aggregate(
    population = transformed,
    variables = feature_cols,
    strata = group_cols
  ) %>%
  dplyr::collect()
```

... and then apply feature selection on the per-cell data. 
```{r}
selected <-
  cytominr::select(
    population = transformed,
    variables = feature_cols,
    sample = aggregated,
    operation = "correlation_threshold"
  ) %>%
  dplyr::collect()
```

And now lets take a glimpse at the data!
```{r}
selected %>%
  dplyr::glimpse()
```

Subset data for easier computation

```{r}
df <- collect(data)
dim(df)
sier computation#
unique(df$Image_Metadata_Plate_DAPI)
dfs <- filter(df, grepl("Week[1|2]_", Image_Metadata_Plate_DAPI))
dim(dfs)
```

Choose our favourite features and compare before and after transformation.

```{r}
feat <- grep("Nuclei_|Cells_|Cytoplasm_", colnames(dfs), value=TRUE)
nFeat <- length(feat)

library(ggplot2)


favFeat <- c("Nuclei_AreaShape_Area", 
             "Cells_Neighbors_NumberOfNeighbors_10",
             "Cells_Intensity_MeanIntensity_CorrActin",
             "Nuclei_Texture_SumEntropy_CorrDAPI_10")
#source("http://www.bioconductor.org/biocLite.R")
#biocLite("tidyr")
library(tidyr)
dfl <- gather(dfs[, favFeat], feature, value)
ggplot(dfl, aes(x=feature, y=value))+facet_wrap(~feature, scale="free")+geom_violin()
dflLog <- mutate(dfl, value=log(value))
ggplot(dflLog, aes(x=feature, y=value))+facet_wrap(~feature, scale="free")+geom_violin()
sum(is.na(dflLog$value) | is.infinite(dflLog$value))
```

Log looks like a good transformation. However over 3,000 cells have been removed. To solve this problem we instead try a generalized log tranform.

```{r}
glog <- function(x, c) log( (x + (x ^ 2 + c ^ 2) ^ 0.5 ) / 2)
glogTransform <- function(x, q=0.05) {
    xq <- quantile(x, q, na.rm=TRUE)
    glog(x, xq)
}
dfsGlog <- dfs %>% mutate_each_(funs(glogTransform), feat)
dflGlog <- gather(dfsGlog[, favFeat], feature, value)
ggplot(dflGlog, aes(x=feature, y=value))+facet_wrap(~feature, scale="free")+geom_violin()
sum(is.na(dflGlog$value) | is.infinite(dflGlog$value))
```

Now we have nicely transformed data without having to throw away any negative values. Of course, not all distributions will benefit equally from the transformation.

```{r}
colnames(dfsGlog[,! colnames(dfsGlog) %in% feat])
dfsGlog$well=dfsGlog$Image_Metadata_Well_DAPI
dfsGlog$plate=dfsGlog$Image_Metadata_Plate_DAPI

ggplot(dfsGlog,aes(y=Cells_Intensity_MeanIntensity_CorrActin,x=well)) + geom_violin()
ggplot(dfsGlog,aes(y=Cells_Intensity_MeanIntensity_CorrActin,x=plate)) + geom_violin()
```

Intensity looks like it does not need any normalization. Are other features the same? Choose to calculate the standard deviation of the plate medians per feature.

```{r}
plateMedianAcrossFeatures <- dfsGlog %>% group_by(plate) %>% summarize_each_(funs(median), feat) 

ftSdAcrossPlateMedians <- plateMedianAcrossFeatures %>% summarize_each_(funs(sd), feat)
ftSdAcrossPlateMedians <- unlist(ftSdAcrossPlateMedians)
ftSdAcrossPlateMedians <- ftSdAcrossPlateMedians[order(ftSdAcrossPlateMedians, decreasing=TRUE)]
dfFt <- data.frame(rank=seq_along(ftSdAcrossPlateMedians), sd=ftSdAcrossPlateMedians)
ggplot(dfFt, aes(x=rank, y=sd))+geom_point()+geom_hline(yintercept=0.3)
dfExclude <- filter(dfFt, sd >= 0.3)
print(dfExclude)
```

We choose to exclude features that have a high variability across screening batches, since we do not expect an entire plate to be significantly different than the others.

Most features have low variability according to the above measure. We do not see the need to perform additional batch normalization.

```{r}
dim(dfsGlog)
dfsGlog <- select(dfsGlog, -one_of(as.character(dfExclude$lab)))
dim(dfsGlog)
```

From there we can assume the dataset to be unbiased so we can perform all further scaling steps on the basis of the entire dataset regardless of well or plate location. There are two ways of performing scaling. The first estimates the median(mean) and mad (sd) on the population of negative controls (here: DMSO). And the second uses the entire dataset. When z-scaling using the negative controls one can infer from the z-score if a perturbation is more or less distant from the negative controls (so its general activity). On the other hand, when using the entire dataset for scaling we can infer information on the significance of the perturbation effect compared to all other perturbations. If the assumption hold true that most compounds of the library are inactive both approaches lead to the same result.

Where are the negative controls?

```{r}
colnames(dfsGlog)[! colnames(dfsGlog) %in% feat]
unique(dfsGlog$Image_Metadata_Compound)
dfDMSO <- filter(dfsGlog, Image_Metadata_Compound == "DMSO")
ggplot(dfDMSO,aes(y=Cells_Intensity_MeanIntensity_CorrActin,x=plate)) + geom_violin()
unique(dfDMSO$well)
```

```{r}
dfsGlog$isDMSO <- dfsGlog$Image_Metadata_Compound == "DMSO"
ggplot(dfsGlog, aes(x=Cytoplasm_Zernike_4_0, col=isDMSO))+geom_density()
feat <- feat[! feat %in% dfExclude$lab]
controlMedian <- dfsGlog %>% group_by(isDMSO) %>% summarize_each_(funs(median), feat)
controlDiff <- controlMedian %>% summarize_each_(funs(diff), feat)
controlDiff <- unlist(controlDiff)
controlDiff <- controlDiff[order(controlDiff, decreasing=TRUE)]
head(controlDiff, 20)
```

DMSO appears to be mimicking the behaviour of the other wells.

```{r}
zDMSO <- function(x, isDMSO) (x-median(x[isDMSO], na.rm=TRUE))/(mad(x[isDMSO], na.rm=TRUE)*1.48)

#lapply(split(dfsGlog,plate),function(x){apply(2,zDMSO,)

dfDMSO <- dfsGlog %>% group_by(plate) %>% mutate_each_(funs(zDMSO(., isDMSO)), feat)
```
